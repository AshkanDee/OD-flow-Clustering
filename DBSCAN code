install.packages(c("dbscan", "ggplot2", "dplyr"))
library(dbscan)
library(ggplot2)
library(dplyr)
############################################################
# Step 1 — Load + inspect all cities (no combining)
#
# WHY:
# - Quick schema check across cities in one run
# - Confirms consistency before you start clustering
# - Does NOT merge/stack datasets (each city stays separate)
############################################################

suppressPackageStartupMessages({
  library(data.table)
})

city_paths <- list(
  berlin  = "/content/drive/MyDrive/berlin_flows_utm.rds",
  munich  = "/content/drive/MyDrive/munich_flows_utm.rds",
  cologne = "/content/drive/MyDrive/cologne_flows_utm.rds"
)

# Store each city separately (no combining)
city_data <- lapply(names(city_paths), function(city_id) {
  dt <- readRDS(city_paths[[city_id]])
  dt <- as.data.table(dt)
  dt[, city := city_id]  # traceability tag
  dt
})
names(city_data) <- names(city_paths)

# ---- Print inspection for each city
for (city_id in names(city_data)) {
  dt <- city_data[[city_id]]

  cat("\n=============================\n")
  cat("CITY:", toupper(city_id), "\n")
  cat("=============================\n")

  cat("class():\n")
  print(class(dt))

  cat("\ndim():\n")
  print(dim(dt))

  cat("\nnames():\n")
  print(names(dt))
}

############################################################
# Step 2 — Structural correction + scaling (per city)
#
# WHAT this step produces:
# - X_scaled: scaled feature matrix for clustering
# - city_clean: filtered dataset aligned with X_scaled rows
#
# Key decisions (explicit WHY):
# - We drop rows with NA in clustering features (complete.cases)
#   because distance-based clustering cannot handle missing values reliably.
# - We scale features because DBSCAN/HDBSCAN use distance computations;
#   scaling prevents any one dimension from dominating due to units/magnitude.
############################################################

suppressPackageStartupMessages({
  library(dplyr)
})

make_X_and_clean <- function(city_dt,
                            feature_cols = c("x_o", "y_o", "dx", "dy")) {

  # WHY: fail fast if a city has a different schema (examiner-friendly)
  missing_cols <- setdiff(feature_cols, names(city_dt))
  if (length(missing_cols) > 0) {
    stop("Missing required columns: ", paste(missing_cols, collapse = ", "))
  }

  # Build feature matrix (same as your berlin snippet, just generalized)
  X <- city_dt %>%
    dplyr::select(dplyr::all_of(feature_cols)) %>%
    as.matrix()

  # Drop rows with any NA in features
  ok <- complete.cases(X)

  # Scale only the valid rows
  X_scaled <- scale(X[ok, , drop = FALSE])

  # Keep dataset aligned with X_scaled rows
  city_clean <- city_dt[ok, ]

  list(X = X, ok = ok, X_scaled = X_scaled, city_clean = city_clean)
}

# ---- Run for all cities (no combining)
# Assumes you already created: city_data$berlin, city_data$munich, city_data$cologne
prepared <- lapply(names(city_data), function(city_id) {
  res <- make_X_and_clean(city_data[[city_id]])
  res$city_id <- city_id
  res
})
names(prepared) <- names(city_data)

# ---- Optional: keep Berlin-style objects for your existing code compatibility
berlin_clean  <- prepared$berlin$city_clean
munich_clean  <- prepared$munich$city_clean
cologne_clean <- prepared$cologne$city_clean

X_scaled_berlin  <- prepared$berlin$X_scaled
X_scaled_munich  <- prepared$munich$X_scaled
X_scaled_cologne <- prepared$cologne$X_scaled

# Quick audit print (helps you catch NA issues early)
for (city_id in names(prepared)) {
  cat("\n---", toupper(city_id), "---\n")
  cat("Rows original:", nrow(city_data[[city_id]]), "\n")
  cat("Rows kept:",     nrow(prepared[[city_id]]$city_clean), "\n")
  cat("Rows dropped:",  sum(!prepared[[city_id]]$ok), "\n")
  cat("X_scaled dim:",  paste(dim(prepared[[city_id]]$X_scaled), collapse = " x "), "\n")
}
############################################################
# Step 3 — kNN-distance plot configuration
#
# WHY this step:
# - kNN-distance plots are used to visually assess a reasonable
#   neighborhood scale (eps) for density-based clustering.
# - We inspect multiple k values to understand sensitivity
#   across different minimum neighborhood sizes.
#
# IMPORTANT:
# - These k values are diagnostic only (not optimization yet).
# - They are aligned with minPts values later used in DBSCAN/HDBSCAN
#   for consistency and interpretability.
############################################################

# Chosen k values (same across all cities)
# WHY these values:
# - 10   : captures local dense patterns
# - 20–30: moderate neighborhood size (often stable)
# - 50   : stress-test for over-smoothing
k_values <- c(10, 20, 30, 50)
############################################################
# Step 3a — Plot layout setup (kNN-distance diagnostics)
#
# WHY this step:
# - We visualize multiple kNN-distance plots side-by-side
#   to compare neighborhood scales for different k values.
# - A fixed 2×2 layout is sufficient for the chosen k grid (4 values).
#
# SAFETY:
# - We store the original graphical parameters and restore them later
#   to avoid side effects on subsequent plots.
############################################################

# Save current graphical parameters
old_par <- par(no.readonly = TRUE)

# Set plotting layout (one panel per k value)
par(mfrow = c(2, 2))
# Restore original graphics settings
par(old_par)
############################################################
# Step 4 — DBSCAN parameter grids
# WHY: explore sensitivity of density scale (eps) and minPts
############################################################

# Neighborhood radius candidates
eps_grid <- c(0.2, 0.3, 0.4, 0.5, 0.6)

# minPts candidates (aligned with kNN diagnostics)
k_grid   <- c(20, 30, 50)
############################################################
# Step 5 — Initialize results container
# WHY: collect clustering outputs across cities and parameters
############################################################

results <- list()
############################################################
# Step 5a — Result index counter
# WHY: sequentially store results in a preallocated list
############################################################

idx <- 1
############################################################
# Step 5b — Sanity checks / debugging prints
# WHY: verify grids, counters, and data alignment before DBSCAN
############################################################

# Index counter
idx

# Parameter grids
k_values
eps_grid
k_grid

# Results container (should be empty at this stage)
results

# Check feature matrices per city (first rows only)
for (city_id in names(prepared)) {
  cat("\n---", toupper(city_id), "---\n")
  cat("X head:\n")
  print(head(prepared[[city_id]]$X))
  cat("X_scaled head:\n")
  print(head(prepared[[city_id]]$X_scaled))
}

############################################################
# Step 7 — Final DBSCAN parameters (per city)
############################################################

dbscan_params <- list(
  berlin  = list(eps = 0.2, minPts = 30),
  cologne = list(eps = 0.3, minPts = 30),
  munich  = list(eps = 0.2, minPts = 30)
)
############################################################
# Step 7a — Final DBSCAN run (all cities)
############################################################

for (city_id in names(prepared)) {

  params <- dbscan_params[[city_id]]

  db_final <- dbscan(
    prepared[[city_id]]$X_scaled,
    eps    = params$eps,
    minPts = params$minPts
  )

  prepared[[city_id]]$city_clean$cluster <- db_final$cluster  # 0 = noise

  cat(
    "DBSCAN done |",
    city_id,
    "| eps =", params$eps,
    "| minPts =", params$minPts,
    "\n"
  )
}
############################################################
# Step 8 — Verify final clustering (cluster size check)
############################################################

for (city_id in names(prepared)) {

  cat("\n---", toupper(city_id), "---\n")

  cl_tab <- table(prepared[[city_id]]$city_clean$cluster)

  # Print first clusters (including noise = 0)
  print(head(cl_tab, 65))
}
berlin_clean  <- prepared$berlin$city_clean
munich_clean  <- prepared$munich$city_clean
cologne_clean <- prepared$cologne$city_clean
############################################################
# Step 9 — Cluster summary (size + means)
############################################################

min_cluster_size <- 10  # small clusters are unstable patterns

cluster_summaries <- list()

for (city_id in names(prepared)) {

  city_dt <- prepared[[city_id]]$city_clean

  cluster_summary <- city_dt %>%
    filter(cluster != 0) %>%                 # exclude noise
    group_by(cluster) %>%
    summarise(
      size = n(),
      mean_x_o = mean(x_o),
      mean_y_o = mean(y_o),
      mean_dx  = mean(dx),
      mean_dy  = mean(dy),
      mean_len = mean(len, na.rm = TRUE),
      mean_duration = mean(duration, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    filter(size >= min_cluster_size) %>%     # APPLY FILTER HERE
    arrange(desc(size))

  cluster_summaries[[city_id]] <- cluster_summary

  cat("\n---", toupper(city_id), "---\n")
  print(head(cluster_summary, 15))
}
############################################################
# Step 11 — Exclude dominant cluster (if needed)
############################################################

moo_clusters <- list()

for (city_id in names(cluster_summaries)) {

  cs <- cluster_summaries[[city_id]]

  # Identify dominant cluster by size
  dominant_cluster <- cs$cluster[which.max(cs$size)]

  cs_moo <- cs %>%
    dplyr::filter(cluster != dominant_cluster)

  moo_clusters[[city_id]] <- cs_moo

  cat(
    city_id,
    "| dominant cluster:", dominant_cluster,
    "| clusters kept:", nrow(cs_moo), "\n"
  )
}
# To prevent a single city-wide cluster from masking secondary patterns,
the dominant cluster (by size) is excluded from the multi-objective analysis when it accounts for a disproportionate share of flows.

############################################################
# Step 12 — Number of clusters used for MOO
############################################################

for (city_id in names(moo_clusters)) {

  cluster_ids <- moo_clusters[[city_id]]$cluster
  K <- length(cluster_ids)

  cat("CITY:", toupper(city_id), "| K =", K, "\n")
}

############################################################
# Step 13 — Directional similarity matrix (per city)
############################################################

direction_similarity <- list()

for (city_id in names(moo_clusters)) {

  cs <- moo_clusters[[city_id]]

  # Unit direction vectors
  v <- as.matrix(cs[, c("mean_dx", "mean_dy")])
  norms <- sqrt(rowSums(v^2))
  norms[norms == 0] <- 1          # safety
  u <- v / norms

  # Cosine similarity matrix (K x K)
  S <- u %*% t(u)
  diag(S) <- NA                  # ignore self-similarity

  direction_similarity[[city_id]] <- S

  cat(
    "CITY:", toupper(city_id),
    "| K =", nrow(S),
    "| similarity matrix computed\n"
  )
}
############################################################
# Step 14 — Weighted direction concentration
############################################################

dir_concentration_weighted <- function(sel, u, w) {
  if (length(sel) < 2) return(0)

  w_sel <- w[sel]
  if (!is.finite(sum(w_sel)) || sum(w_sel) <= 0) return(0)

  ww <- w_sel / sum(w_sel)
  m  <- colSums(u[sel, , drop = FALSE] * ww)

  sqrt(sum(m^2))
}
############################################################
# Step 15 — Evaluation function (two objectives, both minimized)
############################################################

eval_solution <- function(x, df, u) {
  sel <- which(x == 1)

  # Objective 1: maximize total size -> minimize negative size
  total_size <- if (length(sel) == 0) 0 else sum(df$size[sel], na.rm = TRUE)
  f1 <- -total_size

  # Objective 2: maximize directional concentration -> minimize negative concentration
  dir_conc <- dir_concentration_weighted(sel, u, df$size)

  # Penalty: require at least 2 clusters to define concentration meaningfully
  f2 <- if (length(sel) < 2) 1 else -dir_conc

  c(
    f1_neg_size = f1,
    f2_neg_dir_concentration = f2
  )
}
############################################################
# Step 15a — Test evaluation function (per city)
############################################################

for (city_id in names(moo_clusters)) {

  df <- moo_clusters[[city_id]]
  u  <- direction_similarity[[city_id]]  # actually unit vectors stored earlier? careful
  # ⚠️ IMPORTANT: u here must be the UNIT DIRECTION VECTORS, not S
}
direction_vectors <- list()
direction_similarity <- list()

for (city_id in names(moo_clusters)) {

  cs <- moo_clusters[[city_id]]

  v <- as.matrix(cs[, c("mean_dx", "mean_dy")])
  norms <- sqrt(rowSums(v^2))
  norms[norms == 0] <- 1
  u <- v / norms

  S <- u %*% t(u)
  diag(S) <- NA

  direction_vectors[[city_id]]   <- u
  direction_similarity[[city_id]] <- S
}
for (city_id in names(moo_clusters)) {

  df <- moo_clusters[[city_id]]
  u  <- direction_vectors[[city_id]]

  x <- rep(1, nrow(df))   # select all clusters

  res <- eval_solution(x, df, u)

  cat(
    "CITY:", toupper(city_id),
    "| f1 =", res[1],
    "| f2 =", res[2], "\n"
  )
}
############################################################
# Step 15b — Sanity-check extreme solutions
############################################################

for (city_id in names(moo_clusters)) {

  df <- moo_clusters[[city_id]]
  u  <- direction_vectors[[city_id]]
  K  <- nrow(df)

  cat("\nCITY:", toupper(city_id), "\n")

  # Select no clusters
  print(eval_solution(rep(0, K), df, u))

  # Select all clusters
  print(eval_solution(rep(1, K), df, u))
}
############################################################
# Step 15b — Sanity checks (per city)
############################################################

for (city_id in names(moo_clusters)) {

  df <- moo_clusters[[city_id]]
  u  <- direction_vectors[[city_id]]
  K  <- nrow(df)

  cat("\nCITY:", toupper(city_id), "\n")

  # select none
  print(eval_solution(rep(0, K), df, u))

  # select all
  print(eval_solution(rep(1, K), df, u))
}
############################################################
# Step 15c — Sanity check with random selections (per city)
############################################################

set.seed(1)

for (city_id in names(moo_clusters)) {

  df <- moo_clusters[[city_id]]
  u  <- direction_vectors[[city_id]]
  K  <- nrow(df)

  cat("\nCITY:", toupper(city_id), "\n")

  for (i in 1:5) {
    x <- rbinom(K, 1, 0.2)
    print(eval_solution(x, df, u))
  }
}
############################################################
# Step 16 — Random solution sampling (per city)
############################################################

set.seed(1)
N <- 500

vals_samples <- list()

for (city_id in names(moo_clusters)) {

  df <- moo_clusters[[city_id]]
  u  <- direction_vectors[[city_id]]
  K  <- nrow(df)

  vals <- matrix(NA, nrow = N, ncol = 2)

  for (i in 1:N) {
    x <- rbinom(K, 1, 0.2)
    vals[i, ] <- eval_solution(x, df, u)
  }

  vals_df <- data.frame(
    city = city_id,
    covered_size = -vals[, 1],
    dir_concentration = -vals[, 2]
  )

  vals_samples[[city_id]] <- vals_df

  cat("\nCITY:", toupper(city_id), "\n")
  print(summary(vals_df))
}
############################################################
# Step 17 — Scatter plot of random solutions (per city)
############################################################

for (city_id in names(vals_samples)) {

  vals_df <- vals_samples[[city_id]]

  plot(
    vals_df$covered_size,
    vals_df$dir_concentration,
    xlab = "Covered demand (sum size)",
    ylab = "Direction concentration",
    main = paste("Random solutions |", toupper(city_id))
  )
}
install.packages("mco")
library(mco)
############################################################
# Step 18 — Objective function wrapper (per city)
############################################################

objfun_list <- list()

for (city_id in names(moo_clusters)) {

  df <- moo_clusters[[city_id]]
  u  <- direction_vectors[[city_id]]

  objfun_list[[city_id]] <- function(x_cont) {
    x_bin <- as.integer(x_cont > 0.5)
    eval_solution(x_bin, df, u)
  }
}
city_id <- "berlin"
K <- nrow(moo_clusters[[city_id]])

x_test <- runif(K)
objfun_list[[city_id]](x_test)
############################################################
# Step 19 — NSGA-II optimization (per city)
############################################################

set.seed(1)

nsga_results <- list()

for (city_id in names(objfun_list)) {

  K <- nrow(moo_clusters[[city_id]])

  res <- nsga2(
    fn = objfun_list[[city_id]],
    idim = K,
    odim = 2,
    lower.bounds = rep(0, K),
    upper.bounds = rep(1, K),
    popsize = 200,
    generations = 200
  )

  nsga_results[[city_id]] <- res

  cat("NSGA-II finished for:", toupper(city_id), "\n")
}
direction_vectors <- list()

for (city_id in names(moo_clusters)) {
  cs <- moo_clusters[[city_id]]

  v <- as.matrix(cs[, c("mean_dx", "mean_dy")])
  norms <- sqrt(rowSums(v^2))
  norms[norms == 0] <- 1
  u <- v / norms

  direction_vectors[[city_id]] <- u
}
objfun_list <- list()

for (city_id in names(moo_clusters)) {

  df <- moo_clusters[[city_id]]
  u  <- direction_vectors[[city_id]]

  objfun_list[[city_id]] <- local({
    df_local <- df
    u_local  <- u

    function(x_cont) {
      x_bin <- as.integer(x_cont > 0.5)
      eval_solution(x_bin, df_local, u_local)
    }
  })
}
set.seed(1)
nsga_results <- list()

for (city_id in names(objfun_list)) {

  K <- nrow(moo_clusters[[city_id]])
  cat("Running NSGA-II |", city_id, "| K =", K, "\n")

  nsga_results[[city_id]] <- nsga2(
    fn = objfun_list[[city_id]],
    idim = K,
    odim = 2,
    lower.bounds = rep(0, K),
    upper.bounds = rep(1, K),
    popsize = 200,
    generations = 200
  )
}
for (city_id in names(nsga_results)) {

  F <- nsga_results[[city_id]]$value

  pareto_df <- data.frame(
    covered_demand     = -F[, 1],
    dir_concentration  = -F[, 2]
  )

  plot(pareto_df$covered_demand, pareto_df$dir_concentration,
       xlab = "Covered demand (sum size)",
       ylab = "Direction concentration",
       main = paste("Pareto front |", toupper(city_id)))
}
for (city_id in names(moo_clusters)) {
  df <- moo_clusters[[city_id]]
  cat(city_id, ": K =", nrow(df),
      "| total clustered size =", sum(df$size), "\n")
}
############################################################
# Step 21 — Pareto front summary (per city, DBSCAN)
############################################################

pareto_summaries <- list()

for (city_id in names(nsga_results)) {

  F <- nsga_results[[city_id]]$value

  pareto_df <- data.frame(
    covered_demand    = -F[, 1],
    dir_concentration = -F[, 2]
  )

  pareto_summaries[[city_id]] <- pareto_df

  cat("\n---", toupper(city_id), "---\n")
  print(summary(pareto_df))
}
############################################################
# Step 22 — Knee point selection on Pareto front (per city)
############################################################

knee_points <- list()

for (city_id in names(pareto_summaries)) {

  pareto_df <- pareto_summaries[[city_id]]

  x <- pareto_df$covered_demand
  y <- pareto_df$dir_concentration

  # Normalize objectives to [0,1]
  x_n <- (x - min(x)) / (max(x) - min(x))
  y_n <- (y - min(y)) / (max(y) - min(y))

  # Distance to ideal point (1,1)
  dist <- sqrt((1 - x_n)^2 + (1 - y_n)^2)

  knee_idx <- which.min(dist)

  knee_points[[city_id]] <- pareto_df[knee_idx, ]

  cat("\n---", toupper(city_id), "---\n")
  cat("Knee index:", knee_idx, "\n")
  print(pareto_df[knee_idx, ])
}
############################################################
# Step 23 — Selected clusters at knee point (per city)
############################################################

selected_clusters_by_city <- list()

for (city_id in names(nsga_results)) {

  res <- nsga_results[[city_id]]
  df  <- moo_clusters[[city_id]]

  pareto_df <- pareto_summaries[[city_id]]

  # Recompute knee_idx consistently (same logic as Step 22)
  x <- pareto_df$covered_demand
  y <- pareto_df$dir_concentration
  x_n <- (x - min(x)) / (max(x) - min(x))
  y_n <- (y - min(y)) / (max(y) - min(y))
  dist <- sqrt((1 - x_n)^2 + (1 - y_n)^2)
  knee_idx <- which.min(dist)

  # Selected clusters from NSGA-II solution at knee
  x_knee <- as.integer(res$par[knee_idx, ] > 0.5)
  selected_clusters <- df$cluster[x_knee == 1]

  selected_clusters_by_city[[city_id]] <- selected_clusters

  cat("\n---", toupper(city_id), "---\n")
  cat("Selected clusters:", length(selected_clusters), "\n")
  print(selected_clusters)
}
############################################################
# Step 24 — Extreme and knee Pareto solutions (per city)
############################################################

pareto_extremes <- list()

for (city_id in names(pareto_summaries)) {

  pareto_df <- pareto_summaries[[city_id]]

  # Indices of extreme solutions
  best_dir_idx <- which.max(pareto_df$dir_concentration)
  best_cov_idx <- which.max(pareto_df$covered_demand)

  # Recompute knee index (same as Step 22)
  x <- pareto_df$covered_demand
  y <- pareto_df$dir_concentration
  x_n <- (x - min(x)) / (max(x) - min(x))
  y_n <- (y - min(y)) / (max(y) - min(y))
  dist <- sqrt((1 - x_n)^2 + (1 - y_n)^2)
  knee_idx <- which.min(dist)

  pareto_extremes[[city_id]] <- list(
    best_direction = pareto_df[best_dir_idx, ],
    best_coverage  = pareto_df[best_cov_idx, ],
    knee_solution  = pareto_df[knee_idx, ]
  )

  cat("\n---", toupper(city_id), "---\n")
  cat("Best direction solution:\n")
  print(pareto_df[best_dir_idx, ])
  cat("Best coverage solution:\n")
  print(pareto_df[best_cov_idx, ])
  cat("Knee solution:\n")
  print(pareto_df[knee_idx, ])
}
############################################################
# Step 25 — Decode NSGA-II solutions into cluster IDs (per city)
############################################################

decode_solution <- function(sol_idx, res, df) {
  x_bin <- as.integer(res$par[sol_idx, ] > 0.5)
  df$cluster[x_bin == 1]
}

decoded_clusters <- list()

for (city_id in names(nsga_results)) {

  res <- nsga_results[[city_id]]
  df  <- moo_clusters[[city_id]]
  pareto_df <- pareto_summaries[[city_id]]

  # Indices (same logic as Step 24)
  best_dir_idx <- which.max(pareto_df$dir_concentration)
  best_cov_idx <- which.max(pareto_df$covered_demand)

  x <- pareto_df$covered_demand
  y <- pareto_df$dir_concentration
  x_n <- (x - min(x)) / (max(x) - min(x))
  y_n <- (y - min(y)) / (max(y) - min(y))
  knee_idx <- which.min(sqrt((1 - x_n)^2 + (1 - y_n)^2))

  clusters_best_dir <- decode_solution(best_dir_idx, res, df)
  clusters_best_cov <- decode_solution(best_cov_idx, res, df)
  clusters_knee     <- decode_solution(knee_idx,     res, df)

  decoded_clusters[[city_id]] <- list(
    best_direction = clusters_best_dir,
    best_coverage  = clusters_best_cov,
    knee_solution  = clusters_knee
  )

  cat("\n---", toupper(city_id), "---\n")
  cat("Best direction | n =", length(clusters_best_dir), "\n")
  print(clusters_best_dir)
  cat("Best coverage  | n =", length(clusters_best_cov), "\n")
  print(clusters_best_cov)
  cat("Knee solution  | n =", length(clusters_knee), "\n")
  print(clusters_knee)
}
############################################################
# Step 26 — Publishable printing of decoded cluster IDs (per city)
############################################################

print_vec <- function(v) {
  if (length(v) == 0) {
    cat("(none)\n")
  } else {
    cat(paste(v, collapse = ", "), "\n")
  }
}

for (city_id in names(decoded_clusters)) {

  cat("\n====================\n")
  cat("CITY:", toupper(city_id), "\n")
  cat("====================\n")

  cat("best_dir clusters:\n")
  print_vec(decoded_clusters[[city_id]]$best_direction)

  cat("best_cov clusters:\n")
  print_vec(decoded_clusters[[city_id]]$best_coverage)

  cat("knee clusters:\n")
  print_vec(decoded_clusters[[city_id]]$knee_solution)
}
############################################################
# Step 27 — Solution tables for selected Pareto solutions (per city)
############################################################

make_solution_table <- function(selected, cluster_summary) {
  cluster_summary %>%
    dplyr::filter(cluster %in% selected) %>%
    dplyr::select(
      cluster,
      size,
      mean_dx,
      mean_dy,
      mean_len,
      mean_duration
    ) %>%
    dplyr::arrange(dplyr::desc(size))
}

solution_tables <- list()

for (city_id in names(decoded_clusters)) {

  cs <- cluster_summaries[[city_id]]

  tab_best_dir <- make_solution_table(
    decoded_clusters[[city_id]]$best_direction,
    cs
  )

  tab_knee <- make_solution_table(
    decoded_clusters[[city_id]]$knee_solution,
    cs
  )

  tab_best_cov <- make_solution_table(
    decoded_clusters[[city_id]]$best_coverage,
    cs
  )

  solution_tables[[city_id]] <- list(
    best_direction = tab_best_dir,
    knee_solution  = tab_knee,
    best_coverage  = tab_best_cov
  )

  cat("\n====================\n")
  cat("CITY:", toupper(city_id), "\n")
  cat("====================\n")

  cat("\nBest direction solution (top 10 clusters):\n")
  print(head(tab_best_dir, 10))

  cat("\nKnee solution (top 10 clusters):\n")
  print(head(tab_knee, 10))

  cat("\nBest coverage solution (top 10 clusters):\n")
  print(head(tab_best_cov, 10))
}

############################################################
# Step 28 — Covered demand of selected Pareto solutions (per city)
############################################################

covered_demand <- list()

for (city_id in names(solution_tables)) {

  tab_best_dir <- solution_tables[[city_id]]$best_direction
  tab_knee     <- solution_tables[[city_id]]$knee_solution
  tab_best_cov <- solution_tables[[city_id]]$best_coverage

  cov_best_dir <- sum(tab_best_dir$size)
  cov_knee     <- sum(tab_knee$size)
  cov_best_cov <- sum(tab_best_cov$size)

  covered_demand[[city_id]] <- c(
    best_direction = cov_best_dir,
    knee_solution  = cov_knee,
    best_coverage  = cov_best_cov
  )

  cat("\n====================\n")
  cat("CITY:", toupper(city_id), "\n")
  cat("====================\n")
  cat("Covered demand (sum of cluster sizes):\n")
  cat("  Best direction :", cov_best_dir, "\n")
  cat("  Knee solution  :", cov_knee, "\n")
  cat("  Best coverage  :", cov_best_cov, "\n")
}

############################################################
# Step 29 — Visualize knee solution flow corridors (per city)
############################################################

library(ggplot2)

for (city_id in names(solution_tables)) {

  cs  <- cluster_summaries[[city_id]]
  sel <- decoded_clusters[[city_id]]$knee_solution

  plot_df <- cs %>%
    dplyr::filter(cluster %in% sel) %>%
    dplyr::mutate(
      x1 = mean_x_o,
      y1 = mean_y_o,
      x2 = mean_x_o + mean_dx,
      y2 = mean_y_o + mean_dy
    )

  p <- ggplot(plot_df) +
    geom_segment(
      aes(x = x1, y = y1, xend = x2, yend = y2, linewidth = size),
      arrow = arrow(length = unit(0.15, "cm"))
    ) +
    scale_linewidth_continuous(range = c(0.3, 1.5)) +
    labs(
      title = paste("Knee solution: selected flow corridors |", toupper(city_id)),
      x = "x",
      y = "y"
    ) +
    theme_minimal()

  print(p)
}

############################################################
# Step 30 — Scaled visualization of knee flow corridors (per city)
############################################################

library(ggplot2)

scale_factor <- 0.5   # shorten arrows for visual clarity

for (city_id in names(solution_tables)) {

  cs  <- cluster_summaries[[city_id]]
  sel <- decoded_clusters[[city_id]]$knee_solution

  plot_df <- cs %>%
    dplyr::filter(cluster %in% sel) %>%
    dplyr::mutate(
      x1 = mean_x_o,
      y1 = mean_y_o,
      x2 = mean_x_o + scale_factor * mean_dx,
      y2 = mean_y_o + scale_factor * mean_dy
    )

  p <- ggplot(plot_df) +
    geom_segment(
      aes(x = x1, y = y1, xend = x2, yend = y2, linewidth = size),
      arrow = arrow(length = unit(0.15, "cm"))
    ) +
    coord_equal() +
    scale_linewidth_continuous(range = c(0.3, 1.5)) +
    labs(
      title = paste("Knee solution: selected flow corridors |", toupper(city_id)),
      x = "Easting (m)",
      y = "Northing (m)"
    ) +
    theme_minimal()

  print(p)
}

############################################################
# Step 31 — Percent coverage + cross-city summary table
############################################################

library(dplyr)

# 1) Total clustered demand per city (after noise removal + size>=10 filter, etc.)
total_clustered_by_city <- sapply(names(cluster_summaries), function(city_id) {
  sum(cluster_summaries[[city_id]]$size, na.rm = TRUE)
})

# 2) Build one summary table: absolute + percent coverage for each solution
summary_table <- dplyr::bind_rows(lapply(names(solution_tables), function(city_id) {

  tabs <- solution_tables[[city_id]]

  cov_best_dir <- sum(tabs$best_direction$size, na.rm = TRUE)
  cov_knee     <- sum(tabs$knee_solution$size,  na.rm = TRUE)
  cov_best_cov <- sum(tabs$best_coverage$size,  na.rm = TRUE)

  total_clustered <- total_clustered_by_city[[city_id]]

  data.frame(
    city = city_id,
    total_clustered_demand = total_clustered,

    covered_best_direction = cov_best_dir,
    pct_best_direction = 100 * cov_best_dir / total_clustered,

    covered_knee = cov_knee,
    pct_knee = 100 * cov_knee / total_clustered,

    covered_best_coverage = cov_best_cov,
    pct_best_coverage = 100 * cov_best_cov / total_clustered
  )
}))

# Pretty print
summary_table <- summary_table %>%
  arrange(city)

print(summary_table)

############################################################
# Step HV-RANDOM — Hypervolume of random baseline fronts (normalized)
############################################################

# Build global min/max using BOTH NSGA Pareto + Random samples (fair comparison)
all_for_norm <- do.call(rbind, c(
  lapply(names(pareto_summaries), function(city_id) {
    df <- pareto_summaries[[city_id]][, c("covered_demand","dir_concentration")]
    df
  }),
  lapply(names(vals_samples), function(city_id) {
    df <- vals_samples[[city_id]][, c("covered_demand","dir_concentration")]
    df
  })
))

x_min <- min(all_for_norm$covered_demand, na.rm = TRUE)
x_max <- max(all_for_norm$covered_demand, na.rm = TRUE)
y_min <- min(all_for_norm$dir_concentration, na.rm = TRUE)
y_max <- max(all_for_norm$dir_concentration, na.rm = TRUE)

safe_norm <- function(z, zmin, zmax) {
  if (!is.finite(zmin) || !is.finite(zmax) || isTRUE(all.equal(zmax, zmin))) return(rep(0, length(z)))
  (z - zmin) / (zmax - zmin)
}

ref_min <- c(1.05, 1.05)

hv_random_city <- function(df_city) {
  x_n <- safe_norm(df_city$covered_demand, x_min, x_max)
  y_n <- safe_norm(df_city$dir_concentration, y_min, y_max)
  Fmin <- cbind(1 - x_n, 1 - y_n)
  hv2d_min(Fmin, ref_min) / (1.05^2)   # scaled to [0,1]
}

hv_random_df <- do.call(rbind, lapply(names(vals_samples), function(city_id) {
  df <- vals_samples[[city_id]]
  data.frame(
    city = city_id,
    hv_random_scaled = hv_random_city(df),
    n_points = nrow(df)
  )
}))

print(hv_random_df)

############################################################
# Step HV-COMPARE — Random vs NSGA-II (DBSCAN) hypervolume
############################################################

library(dplyr)
library(tidyr)
library(ggplot2)

hv_nsga_df <- hv_dbscan_df %>%
  dplyr::select(city, hv_nsga_scaled = hypervolume_scaled)

hv_compare <- hv_nsga_df %>%
  left_join(hv_random_df %>% dplyr::select(city, hv_random_scaled), by = "city") %>%
  pivot_longer(cols = c(hv_random_scaled, hv_nsga_scaled),
               names_to = "method", values_to = "hv_scaled") %>%
  mutate(method = recode(method,
                         hv_random_scaled = "Random baseline",
                         hv_nsga_scaled   = "NSGA-II (DBSCAN)"))

ggplot(hv_compare, aes(x = city, y = hv_scaled, fill = method)) +
  geom_col(position = "dodge") +
  labs(
    title = "Hypervolume improvement from random sampling to NSGA-II",
    x = "City",
    y = "Hypervolume (scaled to [0,1])",
    fill = NULL
  ) +
  theme_minimal()

############################################################
# Step HV-DBSCAN — Pure R hypervolume for 2D Pareto fronts
# (works in Colab, no packages)
############################################################

# --- Helper: non-dominated filter for MINIMIZATION
nondominated_min <- function(F) {
  n <- nrow(F)
  keep <- rep(TRUE, n)
  for (i in seq_len(n)) {
    if (!keep[i]) next
    for (j in seq_len(n)) {
      if (i == j || !keep[i]) next
      # j dominates i if j <= i in all and < in at least one
      if (all(F[j, ] <= F[i, ]) && any(F[j, ] < F[i, ])) {
        keep[i] <- FALSE
      }
    }
  }
  F[keep, , drop = FALSE]
}

# --- Helper: exact 2D dominated hypervolume for MINIMIZATION
# Assumes ref is WORSE (larger) than all points in both dimensions.
hv2d_min <- function(F, ref) {
  if (nrow(F) == 0) return(0)

  # Ensure non-dominated and sort by f1 ascending
  F <- nondominated_min(F)
  o <- order(F[, 1], F[, 2])
  F <- F[o, , drop = FALSE]

  hv <- 0
  h  <- ref[2]  # running "ceiling" in f2
  for (i in seq_len(nrow(F))) {
    f1 <- F[i, 1]
    f2 <- F[i, 2]
    if (f2 < h) {
      hv <- hv + (ref[1] - f1) * (h - f2)
      h  <- f2
    }
  }
  hv
}

# --- 1) Build global normalization bounds across ALL cities (DBSCAN)
all_pareto <- do.call(rbind, lapply(names(pareto_summaries), function(city_id) {
  df <- pareto_summaries[[city_id]][, c("covered_demand", "dir_concentration")]
  df <- df[is.finite(df$covered_demand) & is.finite(df$dir_concentration), , drop = FALSE]
  df
}))

x_min <- min(all_pareto$covered_demand, na.rm = TRUE)
x_max <- max(all_pareto$covered_demand, na.rm = TRUE)
y_min <- min(all_pareto$dir_concentration, na.rm = TRUE)
y_max <- max(all_pareto$dir_concentration, na.rm = TRUE)

safe_norm <- function(z, zmin, zmax) {
  if (!is.finite(zmin) || !is.finite(zmax) || isTRUE(all.equal(zmax, zmin))) return(rep(0, length(z)))
  (z - zmin) / (zmax - zmin)
}

# --- 2) Hypervolume per city (normalized)
# Convert maximize (x,y) -> minimize (1-x, 1-y); ref slightly > 1 is safely worse
ref_min <- c(1.05, 1.05)

hv_dbscan_df <- do.call(rbind, lapply(names(pareto_summaries), function(city_id) {

  df <- pareto_summaries[[city_id]][, c("covered_demand", "dir_concentration")]
  df <- df[is.finite(df$covered_demand) & is.finite(df$dir_concentration), , drop = FALSE]

  x_n <- safe_norm(df$covered_demand, x_min, x_max)
  y_n <- safe_norm(df$dir_concentration, y_min, y_max)

  # minimization front
  Fmin <- cbind(f1 = 1 - x_n, f2 = 1 - y_n)

  data.frame(
    city = city_id,
    n_points = nrow(df),
    hypervolume_norm = hv2d_min(Fmin, ref_min)
  )
}))

hv_dbscan_df <- hv_dbscan_df[order(-hv_dbscan_df$hypervolume_norm), ]
print(hv_dbscan_df)

hv_dbscan_df$hypervolume_scaled <- hv_dbscan_df$hypervolume_norm / (1.05^2)
hv_dbscan_df <- hv_dbscan_df[order(-hv_dbscan_df$hypervolume_scaled), ]
print(hv_dbscan_df)

range_df <- dplyr::bind_rows(lapply(names(pareto_summaries), function(city_id) {
  df <- pareto_summaries[[city_id]]
  data.frame(
    city = city_id,
    cov_min = min(df$covered_demand, na.rm = TRUE),
    cov_max = max(df$covered_demand, na.rm = TRUE),
    dir_min = min(df$dir_concentration, na.rm = TRUE),
    dir_max = max(df$dir_concentration, na.rm = TRUE)
  )
}))
print(range_df)

hv_city_scaled <- function(df) {
  x <- df$covered_demand; y <- df$dir_concentration
  x_n <- (x - min(x)) / (max(x) - min(x))
  y_n <- (y - min(y)) / (max(y) - min(y))
  Fmin <- cbind(1 - x_n, 1 - y_n)
  hv <- hv2d_min(Fmin, ref = c(1.05, 1.05))
  hv / (1.05^2)
}

hv_shape_df <- dplyr::bind_rows(lapply(names(pareto_summaries), function(city_id) {
  data.frame(
    city = city_id,
    hypervolume_shape_scaled = hv_city_scaled(pareto_summaries[[city_id]])
  )
})) %>% dplyr::arrange(desc(hypervolume_shape_scaled))

print(hv_shape_df)

############################################################
# Plot — DBSCAN Hypervolume (global-normalized)
############################################################

library(ggplot2)
library(dplyr)

hv_plot_df <- hv_dbscan_df %>%
  mutate(city = factor(city, levels = city[order(hypervolume_scaled, decreasing = TRUE)]))

ggplot(hv_plot_df, aes(x = city, y = hypervolume_scaled)) +
  geom_col(fill = "steelblue") +
  labs(
    title = "DBSCAN Pareto front quality by hypervolume",
    x = "City",
    y = "Hypervolume (scaled to [0,1])"
  ) +
  theme_minimal()

############################################################
# Plot — DBSCAN Hypervolume (city-normalized shape)
############################################################

ggplot(hv_shape_df, aes(x = reorder(city, -hypervolume_shape_scaled),
                         y = hypervolume_shape_scaled)) +
  geom_col(fill = "darkorange") +
  labs(
    title = "DBSCAN Pareto front shape by hypervolume (city-normalized)",
    x = "City",
    y = "Hypervolume (scaled to [0,1])"
  ) +
  theme_minimal()

# Check ranges
range(pareto_summaries$berlin$covered_demand)
range(vals_samples$berlin$covered_demand)

range(pareto_summaries$berlin$dir_concentration)
range(vals_samples$berlin$dir_concentration)
